from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi
from dotenv import load_dotenv
import os
import psycopg2
from psycopg2.extras import RealDictCursor

load_dotenv()

api_key = os.getenv("DASHSCOPE_API_KEY")
database_url = os.getenv("DATABASE_URL")


# ============ 1ï¸âƒ£ æŸ¥è¯¢ Supabase æ•°æ®åº“ ============
def query_students(name=None):
    """æŸ¥è¯¢å­¦ç”Ÿä¿¡æ¯"""
    try:
        conn = psycopg2.connect(database_url)
        cur = conn.cursor(cursor_factory=RealDictCursor)

        if name:
            cur.execute("SELECT * FROM students WHERE name LIKE %s", (f"%{name}%",))
        else:
            cur.execute("SELECT * FROM students LIMIT 10")

        results = cur.fetchall()
        cur.close()
        conn.close()
        return results
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return []


# ============ 2ï¸âƒ£ ä» CherryStudio åŠ è½½çŸ¥è¯†åº“ ============
def load_knowledge_base():
    """ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“ï¼ˆæ¨¡æ‹Ÿ CherryStudio å¯¼å‡ºï¼‰"""
    # å®é™…åº”è¯¥ä» CherryStudio å¯¼å‡ºçš„ JSON/CSV æ–‡ä»¶è¯»å–
    docs = [
        "å­¦ç”Ÿè§„å®šï¼šå­¦ç”Ÿå¿…é¡»éµå®ˆæ ¡è§„æ ¡çºª",
        "æˆç»©è¯„åˆ†ï¼š90-100ä¼˜ç§€ï¼Œ80-89è‰¯å¥½ï¼Œ70-79ä¸­ç­‰",
        "å¥–å­¦é‡‘æ”¿ç­–ï¼šGPAå¤§äº3.5å¯ç”³è¯·å¥–å­¦é‡‘",
        "è¯·å‡è§„åˆ™ï¼šäº‹å‡éœ€æå‰ç”³è¯·ï¼Œç—…å‡éœ€åŒ»è¯",
    ]
    return docs


# ============ 3ï¸âƒ£ åˆ›å»ºå‘é‡åº“ ============
docs = load_knowledge_base()
splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
chunks = splitter.split_text("\n".join(docs))

embeddings = DashScopeEmbeddings(model="text-embedding-v3")
vector_store = FAISS.from_texts(chunks, embeddings)

print(f"âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼Œ{len(chunks)} ä¸ªchunk")


# ============ 4ï¸âƒ£ ç»¼åˆæŸ¥è¯¢ï¼šæ–‡æ¡£ + æ•°æ®åº“ ============
def answer_question(query):
    """
    ç»¼åˆæŸ¥è¯¢ï¼š
    1. ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
    2. ä»æ•°æ®åº“æŸ¥è¯¢ç›¸å…³æ•°æ®
    3. LLM ç»¼åˆå›ç­”
    """

    print(f"\nğŸ” æŸ¥è¯¢: {query}")

    # ä»çŸ¥è¯†åº“æ£€ç´¢
    doc_results = vector_store.similarity_search(query, k=2)
    doc_context = "\n".join([doc.page_content for doc in doc_results])

    # ä»æ•°æ®åº“æŸ¥è¯¢
    db_results = query_students()
    db_context = "\n".join([f"- {student['name']}: ç­çº§={student['class']}, æˆç»©={student['grade']}"
                            for student in db_results[:3]])

    # LLM ç»¼åˆå›ç­”
    llm = Tongyi()
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå­¦ç”Ÿç®¡ç†åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{doc_context}

ã€å­¦ç”Ÿæ•°æ®ã€‘
{db_context}

ã€é—®é¢˜ã€‘
{query}

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”ã€‚"""

    answer = llm.invoke(prompt)
    print(f"\nâœ… ç­”æ¡ˆ:\n{answer}")
    return answer


# ============ 5ï¸âƒ£ æµ‹è¯• ============
if __name__ == "__main__":

    # æµ‹è¯•1ï¼šåªæŸ¥çŸ¥è¯†åº“
    print("=" * 50)
    print("ã€æµ‹è¯•1ã€‘æŸ¥è¯¢æˆç»©è¯„åˆ†æ ‡å‡†")
    print("=" * 50)
    answer_question("æˆç»©æ€æ ·è¯„åˆ†ï¼Ÿ")

    # æµ‹è¯•2ï¼šåªæŸ¥æ•°æ®åº“
    print("\n" + "=" * 50)
    print("ã€æµ‹è¯•2ã€‘æŸ¥è¯¢å­¦ç”Ÿä¿¡æ¯")
    print("=" * 50)
    students = query_students()
    print(f"âœ… æŸ¥è¯¢åˆ° {len(students)} ä¸ªå­¦ç”Ÿ")
    for s in students[:3]:
        print(f"  - {s['name']}: {s['email']}")

    # æµ‹è¯•3ï¼šç»¼åˆæŸ¥è¯¢
    print("\n" + "=" * 50)
    print("ã€æµ‹è¯•3ã€‘ç»¼åˆæŸ¥è¯¢ï¼ˆçŸ¥è¯†åº“ + æ•°æ®åº“ï¼‰")
    print("=" * 50)
    answer_question("æˆ‘ä»¬å­¦æ ¡æœ‰å“ªäº›å­¦ç”Ÿï¼Ÿå¥–å­¦é‡‘æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")