from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_community.llms import Tongyi
from dotenv import load_dotenv
import os
import psycopg2
from psycopg2.extras import RealDictCursor
import json

load_dotenv()

api_key = os.getenv("DASHSCOPE_API_KEY")
database_url = os.getenv("DATABASE_URL")


# ============ 1ï¸âƒ£ ä» CherryStudio åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶ ============
def load_knowledge_base_from_files():
    """
    ä»æœ¬åœ° TXT æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“
    ä½ çš„ç»“æ„ï¼šç­çº§.txt å’Œ æ–°é—».txt
    """
    docs = []

    # æ–‡ä»¶åˆ—è¡¨ï¼ˆæ”¹æˆä½ çš„å®é™…æ–‡ä»¶ï¼‰
    file_paths = [
        "data/æ–°é—».txt",  # æ–°é—»å†…å®¹
    ]

    for file_path in file_paths:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                docs.append(content)
                print(f"âœ… åŠ è½½: {file_path}")
        except FileNotFoundError:
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    return docs


# ============ 2ï¸âƒ£ ä» CherryStudio å¯¼å‡ºçš„ JSON åŠ è½½ ============
def load_from_cherrystudio_export(json_file):
    """
    å¦‚æœä½ ä» CherryStudio å¯¼å‡ºä¸º JSON
    """
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        docs = []
        for item in data:
            # æ ¹æ®å®é™…ç»“æ„è°ƒæ•´å­—æ®µå
            content = item.get('content') or item.get('text') or item.get('æ–‡æœ¬')
            if content:
                docs.append(content)

        print(f"âœ… ä» JSON åŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
        return docs
    except Exception as e:
        print(f"âŒ åŠ è½½ JSON å¤±è´¥: {e}")
        return []


# ============ 3ï¸âƒ£ æŸ¥è¯¢ Supabase æ•°æ®åº“ ============
def query_students(name=None):
    """æŸ¥è¯¢å­¦ç”Ÿä¿¡æ¯"""
    try:
        conn = psycopg2.connect(database_url)
        cur = conn.cursor(cursor_factory=RealDictCursor)

        if name:
            cur.execute("SELECT * FROM students WHERE name LIKE %s", (f"%{name}%",))
        else:
            cur.execute("SELECT * FROM students LIMIT 10")

        results = cur.fetchall()
        cur.close()
        conn.close()
        return results
    except Exception as e:
        print(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return []


# ============ 4ï¸âƒ£ åˆ›å»ºå‘é‡åº“ ============
def create_vector_store(docs):
    """åˆ›å»º FAISS å‘é‡åº“"""
    if not docs:
        print("âŒ æ²¡æœ‰æ–‡æ¡£æ•°æ®")
        return None

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # å¢å¤§ chunk_sizeï¼Œé€‚åˆè¾ƒé•¿æ–‡æœ¬
        chunk_overlap=100,
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
    )

    chunks = splitter.split_text("\n".join(docs))

    embeddings = DashScopeEmbeddings(model="text-embedding-v3")
    vector_store = FAISS.from_texts(chunks, embeddings)

    print(f"âœ… å‘é‡åº“åˆ›å»ºæˆåŠŸï¼Œ{len(chunks)} ä¸ª chunk")
    return vector_store


# ============ 5ï¸âƒ£ ç»¼åˆæŸ¥è¯¢ï¼šæ–‡æ¡£ + æ•°æ®åº“ ============
def answer_question(query, vector_store):
    """
    ç»¼åˆæŸ¥è¯¢ï¼š
    1. ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
    2. ä»æ•°æ®åº“æŸ¥è¯¢ç›¸å…³æ•°æ®
    3. LLM ç»¼åˆå›ç­”
    """

    print(f"\nğŸ” æŸ¥è¯¢: {query}")
    print("=" * 50)

    # ä»çŸ¥è¯†åº“æ£€ç´¢
    doc_results = vector_store.similarity_search(query, k=3)
    doc_context = "\n".join([f"- {doc.page_content[:200]}" for doc in doc_results])

    # ä»æ•°æ®åº“æŸ¥è¯¢
    db_results = query_students()
    db_context = ""
    if db_results:
        db_context = "\n".join([f"- {student.get('name', 'æœªçŸ¥')}: {str(student)}"
                                for student in db_results[:3]])
    else:
        db_context = "æ•°æ®åº“æš‚æ— å­¦ç”Ÿæ•°æ®"

    # LLM ç»¼åˆå›ç­”
    llm = Tongyi()
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªå­¦æ ¡åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ã€çŸ¥è¯†åº“ä¿¡æ¯ã€‘
{doc_context}

ã€å­¦ç”Ÿæ•°æ®åº“ä¿¡æ¯ã€‘
{db_context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

è¯·ç”¨ç®€æ´çš„ä¸­æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜ã€‚"""

    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...\n")
    answer = llm.invoke(prompt)
    print(f"âœ… ç­”æ¡ˆ:\n{answer}\n")
    return answer


# ============ 6ï¸âƒ£ ä¸»ç¨‹åº ============
if __name__ == "__main__":

    print("ğŸš€ åŠ è½½çŸ¥è¯†åº“...")
    print("=" * 50)

    # æ–¹æ³•1ï¼šä» TXT æ–‡ä»¶åŠ è½½ï¼ˆæ¨èï¼‰
    docs = load_knowledge_base_from_files()

    # æ–¹æ³•2ï¼šä»å¯¼å‡ºçš„ JSON åŠ è½½ï¼ˆå¯é€‰ï¼‰
    # docs = load_from_cherrystudio_export("data/knowledge_base.json")

    if not docs:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        exit(1)

    # åˆ›å»ºå‘é‡åº“
    vector_store = create_vector_store(docs)

    print("\n" + "=" * 50)
    print("âœ… ç³»ç»Ÿå°±ç»ªï¼å¼€å§‹æé—®...")
    print("=" * 50)

    # æµ‹è¯•æŸ¥è¯¢
    questions = [
        "ç­çº§æœ‰å“ªäº›ä¿¡æ¯ï¼Ÿ",
        "æœ€è¿‘æœ‰ä»€ä¹ˆæ–°é—»ï¼Ÿ",
        "æˆ‘ä»¬å­¦æ ¡çš„å­¦ç”Ÿæœ‰å“ªäº›ï¼Ÿ",
        "ç­çº§çš„å…·ä½“æƒ…å†µæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    for question in questions:
        answer_question(question, vector_store)
        print("\n" + "=" * 50)

    # äº¤äº’å¼æŸ¥è¯¢
    print("\nğŸ’¬ è¿›å…¥äº¤äº’æ¨¡å¼ (è¾“å…¥ 'exit' é€€å‡º)")
    print("=" * 50)
    while True:
        user_query = input("\nä½ çš„é—®é¢˜: ").strip()
        if user_query.lower() == 'exit':
            print("ğŸ‘‹ å†è§ï¼")
            break
        if user_query:
            answer_question(user_query, vector_store)